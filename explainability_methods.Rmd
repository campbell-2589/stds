# Explainability Methods

## Sensitivity Analysis and Shapley Values

Global sensitivity analysis measures the importance of input variables to a function. This is an important task in quantifying the uncertainty in which target variables can be predicted from their inputs. Sobol indices [@sobolindices] are a popular approach to this. It turns out that there's a relationship between Sobol indices and Shapley values. We explore this relationship here and demonstrate their effectiveness on some linear and non-linear models. 

## Relationship between Sobol indices and Shapley values 

Shapley values are based on $f(x)-E[f(x)]$ while Sobol indices decompose output variance into fractions contributed by the inputs. The Sobol index is a global measure of feature importance while Shapley values focus on local explanations although we could combine local Shapley values to achieve a global importance measure. Sobol indices are based on expectations and can be used for features not included in the model / function of interest. In this way we could query for important features correlated with those that the model does use.  


## CRAN sensitivity package


```{r,message=FALSE}
library(ggplot2)
library(pander)
if(!require(sensitivity)){
    install.packages("sensitivity")
    library(sensitivity)
}




```


Standardized Regression Coefficients (SRC), or the Standardized Rank Regression Coefficients (SRRC), which are sensitivity indices based on linear or monotonic assumptions in the case of independent factors.

```{r}
n <- 100
X <- data.frame(X1 = runif(n, 0.5, 1.5),
                X2 = runif(n, 1.5, 4.5),
                X3 = runif(n, 4.5, 13.5))

# linear model : Y = X1 + X2 + X3

y <- with(X, X1 + X2 + X3)


Z <- src(X, y, rank = FALSE, logistic = FALSE, nboot = 0, conf = 0.95)

pander(Z$SRC,caption = "Standardized Regression Coefficients ")

ggplot(Z, ylim = c(-1,1))+ggtitle("Standardized Regression Coefficients")

```


```{r}
y <- with(X, X1 + X2 + X3)
y <- y + rnorm(nrow(X),0,1/2)
df<- data.frame(cbind(X,y))

Z <- src(X, y, rank = FALSE, logistic = FALSE, nboot = 0, conf = 0.95)

pander(Z$SRC,caption = "Standardized Regression Coefficients ")

ggplot(Z, ylim = c(-1,1))+ggtitle("Standardized Regression Coefficients")

#lm.fit = lm(y ~ X1+X2+X3,data = df)
#summary(lm.fit)
#attach(df)
#plot(y, X1+X2+X3)
```

We see how the importance of X3 is ranked above X2 and likewise X2 is more important than X1. This is by design of the simulated data set. The standardized regression coefficients (beta coefficients) are calculated from that has been standardized, let's normalize and calculate the regression to see if indeed that is the case. 

```{r}
dfs<- data.frame(scale(df,center = TRUE,scale = TRUE))
lm.fit = lm(y ~ X1+X2+X3,data = dfs)
summary(lm.fit)

```

We see that the values are very close.

## Partial Correlation Coefficients

```{r}

x <- pcc(X, y, nboot = 100)
print(x)
```


## Sobol indices for deterministic function and for model 

```{r}
y.fun <- function(X) {
  
  X1<- X[,1]
  X2<- X[,2]
  X3<- X[,3]
  
  X1+X2+X3
}

yhat.fun<-function(X,lm)
{
  X1<- X[,1]
  X2<- X[,2]
  X3<- X[,3]
  
  yhat <- predict(lm.fit,data.frame(X1=X1,X2=X2,X3=X3))
  return(yhat)
}

nboot = 1000

x <- sobol(model = y.fun, X[1:50,], X[51:100,], order = 3, nboot = nboot)
S.sobol <- x$S
pander(S.sobol)
#yhat.fun(data.frame(X1=1,X2=2,X3=3),lm.fit)

x <- sobol(model = yhat.fun,X[1:50,], X[51:100,], order = 3, nboot = nboot)
S.sobol <- x$S
pander(S.sobol)
```





