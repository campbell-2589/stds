# MLOps

*Development platform: a collaborative platform for performing ML experiments and empowering the creation of ML models by data scientists should be considered part of the MLOps framework. This platform should enable secure access to data sources (e.g. from data engineering workflows). We want the handover from ML training to deployment to be as smooth as possible, which is more likely the case for such a platform as compared to ML models developed in different local environment.
Model unit testing: every time we create, change, or retrain a model, we should automatically validate the integrity of the model, e.g.
- should meet minimum performance on test set
- should perform well on synthetic use case-specific datasets
Versioning: it should be possible to go back in time to inspect everything relating to a given model; e.g. what data & code was used. Why? because it something breaks, we need to be able to go back in time and see why.
Model registry: there should be an overview of deployed & decommissioned ML models, their version history, and deployment stage of each version. Why? if something breaks, we can rollback a previous archived version back into production.
Model Governance: only certain people should have access to see training related to any given model, and there should be access control for who can request/reject/approve transitions between deployment stages (e.g. dev to test to prod) in the model registry.
Deployments: deployment can be many things, but in this post I consider the case where we want to deploy a model to cloud infrastructure and expose an API, which enables other people to consume and use the model, i.e. I’m not considering cases where we want to deploy ML models into embedded systems. Efficient model deployments on appropriate infrastructure should:
- support multiple ML frameworks + custom models
- have well defined API spec (e.g. Swagger/OpenAPI)
- support containerized model servers
Monitoring: tracking performance metrics (throughput, uptime, etc.). Why? If all of the sudden a model starts returning errors, or being unexpectedly slow, we need to know before the end-user complains, so that we can fix it.
Feedback: we need to feedback information to the model on how well it is performing. Why? typically we run predictions on new samples where we do not yet know the ground truth. As we learn the truth, however, we need to inform the model, so that it can report on how well it is actually doing.
A/B testing: no matter how solid cross-validation we think we’re doing, we never know how the model will perform until it actually gets deployed. It should be easy to perform A/B experiments with live models within the MLOps framework.
Drift detection: typically the longer time a given model is deployed the worse it becomes as circumstances change compared to the time of training the model. We can try to monitor and alert on these different circumstances, or “drifts”, before they get too severe:
- Concept drift: when the relation between input and output has changed
- Prediction drift: changes in incoming predictions, but model still holds
- Label drift: change in the model’s outcomes compared to training data
- Feature drift: change in the distribution of model input data
Outlier detection: if a deployed model receives an input sample which is significantly different from anything observed during training, we can try to identify this sample as a potential outlier, and the returned prediction should be marked as such, indicating that the end-user should be careful in trusting the prediction.
Adversarial Attack Detection: we should be warned when our models are attacked by adversarial samples (e.g. someone trying to abuse / manipulate the outcome of our algorithms).
Interpretability: the ML deployments should support endpoints returning the explanation of our prediction, e.g. through SHAP values. Why? for a lot of use cases a prediction is not enough and the end-user needs to know why a given prediction was made.
Governance of deployments: we not only need access restrictions on who can see the data, trained models, etc, but also for who can eventually use the deployed models. These deployed models can often be just as confidential as the data they were trained on.
Data-centricity: rather than focus on model performance & improvements, it makes sense that a MLOps framework also enables an increased focus on how data quality and breadth can be improved.*