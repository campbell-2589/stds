preface
intro
data-science-best-practices-and-processes
code-of-conduct
ethics
the-agile-manifesto
agile-data-science-manifesto
sprints
planning-estimation
execution
demo
retrospectives
open-time
the-master-backlog-and-the-open-business-questions-list
statistical-analysis-vs.-machine-learning-and-r-vs.-python
git-fundamental-ideas
workflow
testing
data-validation
hardware
languages
ides-vs-notebooks
modern-devops
big-data
statistics
theory
applied
machine-learning
core-competencies
to-the-managers-of-data-scientists
when-is-your-data-big
c
on-metrics-and-being-data-driven
fallacies-and-failures-in-judgment
on-relevance
on-model-complexity
on-credit-and-accountability
cognitive-biases
causal-inference
spike-and-slab-regression
nowcasting
higher-criticism
reconciled-distributional-forecasts
crossed-versus-nested-random-effects.
very-large-number-of-res
sensitivity-analysis-and-shapley-values
relationship-between-sobol-indices-and-shapley-values
cran-sensitivity-package
partial-correlation-coefficients
sobol-indices-for-deterministic-function-and-for-model
on-model-averaging
poincare-embedding
embeddings
mlops
introduction-to-normalizing-flows
variational-inference-with-nf
sa-and-m-estimators
random-matrix-theory-and-machine-learning
project-philosophy
project-planning-and-accounting
agile
version-control
definition-of-done-for-data-science
data-science-reading-list
statistics-theory
statistics-applied
mlops---devops-applied-to-machine-learning-workflows
development-platform
versioning
model-governance-only-certain-people-should-have-access-to-see-training-related-to-any-given-model-and-there-should-be-access-control-for-who-can-requestrejectapprove-transitions-between-deployment-stages-e.g.-dev-to-test-to-prod-in-the-model-registry.
deployments-deployment-can-be-many-things-but-in-this-post-i-consider-the-case-where-we-want-to-deploy-a-model-to-cloud-infrastructure-and-expose-an-api-which-enables-other-people-to-consume-and-use-the-model-i.e.-im-not-considering-cases-where-we-want-to-deploy-ml-models-into-embedded-systems.-efficient-model-deployments-on-appropriate-infrastructure-should
feedback-we-need-to-feedback-information-to-the-model-on-how-well-it-is-performing.-why-typically-we-run-predictions-on-new-samples-where-we-do-not-yet-know-the-ground-truth.-as-we-learn-the-truth-however-we-need-to-inform-the-model-so-that-it-can-report-on-how-well-it-is-actually-doing.
ab-testing-no-matter-how-solid-cross-validation-we-think-were-doing-we-never-know-how-the-model-will-perform-until-it-actually-gets-deployed.-it-should-be-easy-to-perform-ab-experiments-with-live-models-within-the-mlops-framework.
drift-detection-typically-the-longer-time-a-given-model-is-deployed-the-worse-it-becomes-as-circumstances-change-compared-to-the-time-of-training-the-model.-we-can-try-to-monitor-and-alert-on-these-different-circumstances-or-drifts-before-they-get-too-severe
outlier-detection-if-a-deployed-model-receives-an-input-sample-which-is-significantly-different-from-anything-observed-during-training-we-can-try-to-identify-this-sample-as-a-potential-outlier-and-the-returned-prediction-should-be-marked-as-such-indicating-that-the-end-user-should-be-careful-in-trusting-the-prediction.
interpretability-the-ml-deployments-should-support-endpoints-returning-the-explanation-of-our-prediction-e.g.-through-shap-values.-why-for-a-lot-of-use-cases-a-prediction-is-not-enough-and-the-end-user-needs-to-know-why-a-given-prediction-was-made.
data-centricity-rather-than-focus-on-model-performance-improvements-it-makes-sense-that-a-mlops-framework-also-enables-an-increased-focus-on-how-data-quality-and-breadth-can-be-improved.
general-advice-thoughts-on-data-science
statistical-methods
higher-criticism-1
hierarchical-and-grouped-time-series
random-effects-and-mixed-models
propensity-score-matching-caliper
explainability-methods
machine-learning-methods
geometric-methods
multirelational
neural-networks
normalizing-flows-variational-inference-with-nf
regularization
section
before_body-frontpage.tex
statistical-methods-1
